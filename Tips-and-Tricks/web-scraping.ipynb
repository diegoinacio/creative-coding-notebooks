{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "selective-opinion",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "---\n",
    "- Author: Diego In√°cio\n",
    "- GitHub: [github.com/diegoinacio](https://github.com/diegoinacio)\n",
    "- Notebook: [web-scraping.ipynb](https://github.com/diegoinacio/creative-coding-notebooks/blob/master/Tips-and-Tricks/web-scraping.ipynb)\n",
    "---\n",
    "Some demonstrations of how to scrape data on the web.\n",
    "\n",
    "[Web scraping](https://en.wikipedia.org/wiki/Web_scraping) or **web data extraction** is a process that allows us to collect structured (or even unstructured) data from the web via requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Display libraries\n",
    "from IPython.display import display, HTML, Image, Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-breed",
   "metadata": {},
   "source": [
    "## Request and HTML Parsing\n",
    "---\n",
    "The libraries we need are:\n",
    "- **Requests**: Allows us to send *HTTP requests* in an extremely easily way.\n",
    "- **Beautiful Soup**: Allows us to extract data from HTML files and parse it to a Python objct.\n",
    "\n",
    "For the following example, let's take the content table from the page [Ordinary Differential Equation](https://en.wikipedia.org/wiki/Ordinary_differential_equation) on wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request and parse\n",
    "URL = \"https://en.wikipedia.org/wiki/Ordinary_differential_equation\"\n",
    "html_text = requests.get(URL).text\n",
    "parse = BeautifulSoup(html_text, \"html.parser\")\n",
    "\n",
    "# Get content table\n",
    "content_table = (\n",
    "    parse\n",
    "        .find(\"div\", {\"id\": \"mw-content-text\"})\n",
    "        .find(\"div\", {\"id\": \"toc\"})\n",
    "        .find(\"ul\")\n",
    ")\n",
    "\n",
    "# Change href for each item to redirect to actual page\n",
    "for a in content_table.find_all(\"a\"):\n",
    "    new_URL = URL + a[\"href\"]\n",
    "    a[\"href\"] = new_URL\n",
    "\n",
    "# Display content\n",
    "HTML(str(content_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-economics",
   "metadata": {},
   "source": [
    "## Structured Data\n",
    "---\n",
    "Scraping structured data with examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-jason",
   "metadata": {},
   "source": [
    "### HTML Tables\n",
    "---\n",
    "Getting data from HTML tables.\n",
    "\n",
    "For the following example, let's take a currency exchange table for Brazillian Real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.x-rates.com/table/?from=BRL&amount=1\"\n",
    "html_text = requests.get(URL).text\n",
    "parse = BeautifulSoup(html_text, \"html.parser\")\n",
    "\n",
    "table = parse.find(\"table\")\n",
    "\n",
    "HTML(str(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-digest",
   "metadata": {},
   "source": [
    "Having the HTML data, we can bind it to a dataframe using *Pandas* library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_table = pd.read_html(str(table))[0]\n",
    "df_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-caribbean",
   "metadata": {},
   "source": [
    "### Consuming open data\n",
    "---\n",
    "We can read a csv file on the web simply using its url as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "URL = \"https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv\"\n",
    "df_csv = pd.read_csv(URL)\n",
    "df_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-purchase",
   "metadata": {},
   "source": [
    "## Unstructured Data\n",
    "---\n",
    "Scraping unstructured data with examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-essex",
   "metadata": {},
   "source": [
    "### NoSQL file\n",
    "---\n",
    "Reading static *json* files from the web. For dynamic APIs would be almost the same process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.plus2net.com/php_tutorial/student.json\"\n",
    "json_file = requests.get(URL).json()\n",
    "\n",
    "json_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-slide",
   "metadata": {},
   "source": [
    "### Image data\n",
    "---\n",
    "Scraping *img* elements on google images search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request | image search for \"zebra\" with large results\n",
    "URL = \"https://www.google.com/search?q=zebra&tbm=isch\"\n",
    "html_text = requests.get(URL).text\n",
    "parse = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "# Find all img and show 5 of them\n",
    "IMG = parse.find_all(\"img\")[1:6]\n",
    "\n",
    "mount = \"\"\n",
    "for img in IMG:\n",
    "    img[\"style\"] = \"float: left\"\n",
    "    mount += str(img)\n",
    "\n",
    "mount = f'<div><h1 style=\"color: red;\">Image Scraping</h1><br>{mount}</div>'\n",
    "\n",
    "HTML(mount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-rabbit",
   "metadata": {},
   "source": [
    "### Audio data\n",
    "---\n",
    "Scraping audio data from web pages.\n",
    "\n",
    "For the following example, let's find all audio elements in a page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "mount = \"\"\n",
    "TITLE, SOURCE = [], []\n",
    "\n",
    "# Request and parsing\n",
    "URL = \"https://en.wikipedia.org/wiki/Additive_synthesis\"\n",
    "html_text = requests.get(URL).text\n",
    "parse = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "# Find all audio elements\n",
    "AUDIO = parse.find_all(\"audio\")\n",
    "\n",
    "# Procedure for each audio element\n",
    "for audio in AUDIO:\n",
    "    title = audio[\"data-mwtitle\"]\n",
    "    source = audio.find(\"source\")\n",
    "    src = source[\"src\"]\n",
    "    mount += f'''\n",
    "    <div>\n",
    "        <h4>{title}</h4><br>\n",
    "        <audio controls>\n",
    "          <source src=\"{src}\" type=\"{source[\"type\"]}\">\n",
    "        Your browser does not support the audio element.\n",
    "        </audio><br>\n",
    "        <a href=\"{src}\">{src}</a>\n",
    "    <div>\n",
    "    '''\n",
    "    TITLE.append(title)\n",
    "    SOURCE.append(src)\n",
    "\n",
    "# Output\n",
    "mount = f'<div><h1 style=\"color: red;\">Audio Scraping</h1><br>{mount}</div>'\n",
    "HTML(mount)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
